{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import logging\n",
    "from train_test import train, evaluate\n",
    "import warnings\n",
    "import json\n",
    "import wandb\n",
    "from arg_parser import init_parser\n",
    "from setproctitle import setproctitle as ptitle\n",
    "from enviroment import ARC_Env\n",
    "import gymnasium as gym\n",
    "from action_space import ARCActionSpace\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "def init_parser(alg):\n",
    "    \"\"\"Initialize argument parser for the specified algorithm.\"\"\"\n",
    "\n",
    "    if alg == 'WOLP_DDPG':\n",
    "        parser = argparse.ArgumentParser(description='WOLP_DDPG')\n",
    "\n",
    "        # Environment & Training Mode\n",
    "        parser.add_argument('--env', default='ARC', metavar='ENV', help='Environment to train on')\n",
    "        parser.add_argument('--mode', default='train', type=str, help='Mode: train/test')\n",
    "        parser.add_argument('--id', default='0', type=str, help='Experiment ID')\n",
    "        parser.add_argument('--load', default=True, metavar='L', help='Load a trained model')\n",
    "        parser.add_argument('--load-model-dir', default='ARC-run67', metavar='LMD', help='Folder to load trained models from')\n",
    "        parser.add_argument('--eval-interval', default=200, type=int, help='Evaluate model every X episodes')\n",
    "        parser.add_argument('--eval-episodes', default=2, type=int, help='Number of episodes to evaluate')\n",
    "\n",
    "        # Episode & Training Settings\n",
    "        parser.add_argument('--max-episode-length', type=int, default=50, metavar='M', help='Max episode length (default: 50)')  # Changed from 1440\n",
    "        parser.add_argument('--max-episode', type=int, default=20000, help='Maximum number of episodes')\n",
    "        parser.add_argument('--max-actions', default=12, type=int, help='# max actions')\n",
    "        parser.add_argument('--test-episode', type=int, default=20, help='Maximum testing episodes')\n",
    "        parser.add_argument('--warmup', default=200, type=int, help='Time without training but only filling the replay memory')\n",
    "        parser.add_argument('--bsize', default=3, type=int, help='Minibatch size')\n",
    "        parser.add_argument('--rmsize', default=100000, type=int, help='Replay memory size')\n",
    "\n",
    "        # Policy Update Settings\n",
    "        parser.add_argument('--gamma', type=float, default=0.99, metavar='G', help='Discount factor for rewards (default: 0.99)')\n",
    "        parser.add_argument('--policy-noise', default=0, type=float, help='Noise added to target policy during critic update')\n",
    "        parser.add_argument('--noise-clip', default=0.25, type=float, help='Range to clip target policy noise')\n",
    "        parser.add_argument('--policy-delay', default=2, type=int, help='Delay policy updates')\n",
    "        parser.add_argument('--c-lr', default=3e-4, type=float, help='Critic network learning rate')\n",
    "        parser.add_argument('--p-lr', default=3e-4, type=float, help='Policy network learning rate (for DDPG)')\n",
    "        parser.add_argument('--tau-update', default=0.0005, type=float, help='Moving average for target network')\n",
    "        parser.add_argument('--weight-decay', default=1e-4, type=float, help='L2 Regularization loss weight decay')\n",
    "        \n",
    "                            \n",
    "        # Neural Network Architecture\n",
    "        parser.add_argument('--hidden1', default=512, type=int, help='Hidden units in the first fully connected layer')\n",
    "        parser.add_argument('--hidden2', default=512, type=int, help='Hidden units in the second fully connected layer')\n",
    "        parser.add_argument('--actor_critic_type', default='cnn', type=str, help='Type of model (lpn, cnn, mlp)')\n",
    "        parser.add_argument('--latent_dim', default=48, type=int, help='Latent dimension for encoder')\n",
    "\n",
    "        # Exploration & Noise\n",
    "        parser.add_argument('--epsilon', default=100000, type=int, help='Linear decay of exploration policy')\n",
    "        parser.add_argument('--epsilon_start', default=1.0, type=float, help='Starting epsilon value for resuming training')\n",
    "        parser.add_argument('--ou_theta', default=0.5, type=float, help='Ornstein-Uhlenbeck noise theta')\n",
    "        parser.add_argument('--ou_sigma', default=0.2, type=float, help='Ornstein-Uhlenbeck noise sigma')\n",
    "        parser.add_argument('--ou_mu', default=0.0, type=float, help='Ornstein-Uhlenbeck noise mu')\n",
    "\n",
    "        # Hardware & GPU Settings\n",
    "        parser.add_argument('--gpu-ids', type=int, default=[1], nargs='+', help='GPUs to use [-1 for CPU only]')\n",
    "        parser.add_argument('--gpu-nums', type=int, default=8, help='Number of GPUs to use (default: 1)')\n",
    "\n",
    "        # Action Embedding & Filtering\n",
    "        parser.add_argument('--load_action_embedding', default=True, type=bool, help='Load action embedding or not')\n",
    "        parser.add_argument('--num_experiments_filter', default=120, type=int, help='Number of problems used for filtering actions')\n",
    "        parser.add_argument('--filter_threshold', default=0.3, type=float, help='Threshold percentage for filtering actions')\n",
    "        parser.add_argument('--num_experiments_similarity', default=120, type=int, help='Number of problems used for similarity matrix calculation')\n",
    "        parser.add_argument('--max_embedding', default=1., type=float, help='Maximum value for embedding matrix')\n",
    "        parser.add_argument('--min_embedding', default=-1., type=float, help='Minimum value for embedding matrix')\n",
    "\n",
    "        # Miscellaneous\n",
    "        parser.add_argument('--init_w', default=0.003, type=float, help='Initial weight')\n",
    "        parser.add_argument('--seed', default=-1, type=int, help='Random seed')\n",
    "        parser.add_argument('--save_per_epochs', default=25, type=int, help='Save model every X epochs')\n",
    "        parser.add_argument('--k_neighbors', default=25, type=int, help='Number of neighbors to consider')\n",
    "        return parser\n",
    "\n",
    "    else:\n",
    "        raise RuntimeError(f'Undefined algorithm {alg}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.argv = ['notebook']  # Replace with a dummy argument list\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print('Using device: {}'.format(device))\n",
    "\n",
    "\n",
    "ptitle('WOLP_DDPG')\n",
    "warnings.filterwarnings('ignore')\n",
    "parser = init_parser('WOLP_DDPG')\n",
    "args = parser.parse_args()\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(args.gpu_ids)[1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Unable to verify login in offline mode.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device for model: mps\n",
      "Using device for EncoderTransformer: mps\n",
      "Using device for memory: mps\n",
      "Using device: mps\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/francescobraicovich/Documents/ARC2 with outputs/ARC2/wandb/run-20250221_143820-cf2t9kzt</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/francesco-braicovich-bocconi-university/arc-v1/runs/cf2t9kzt' target=\"_blank\">soft-lion-95</a></strong> to <a href='https://wandb.ai/francesco-braicovich-bocconi-university/arc-v1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/francesco-braicovich-bocconi-university/arc-v1' target=\"_blank\">https://wandb.ai/francesco-braicovich-bocconi-university/arc-v1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/francesco-braicovich-bocconi-university/arc-v1/runs/cf2t9kzt' target=\"_blank\">https://wandb.ai/francesco-braicovich-bocconi-university/arc-v1/runs/cf2t9kzt</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Creating the action space\n",
      "Number of actions not filtered: 11310\n",
      "Number of actions filtered: 7075\n",
      "NearestNeighbors model created with 25 neighbors\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Creating the action space\n",
      "Number of actions not filtered: 11310\n",
      "Number of actions filtered: 7075\n",
      "NearestNeighbors model created with 25 neighbors\n",
      "--------------------------------------------------\n",
      "[[-0.05092603  0.62167759 -0.80313564 -0.31519486 -0.05130663 -0.35953908\n",
      "  -0.48975622 -0.55581076 -0.57480736 -0.34074036 -0.29114123  0.90078503\n",
      "   0.0063453  -0.4454143  -0.17825451 -0.05588313  0.64388515  0.86552641\n",
      "  -0.08955657  0.7753709 ]\n",
      " [-0.04885672  0.80032061  0.66708647 -0.73917153 -0.10302403  0.59080581\n",
      "   0.62320497  0.07621358 -0.72033781  0.3629343  -0.02611465  0.07564433\n",
      "  -0.53731184 -0.20741292  0.47814419 -0.88411586  0.43059644  0.67943963\n",
      "  -0.06200402 -0.48741514]\n",
      " [-0.40096055  0.24775392  0.69482523  0.75425448 -0.65625216  0.04741802\n",
      "  -0.00910887 -0.01187057  0.6116402   0.68676669  0.54557457 -0.06771539\n",
      "   0.2488829  -0.24646899 -0.08990387 -0.90780779 -0.70003807 -0.02418659\n",
      "  -0.58351821  0.79935908]\n",
      " [ 0.38637632 -0.48902757  0.35099516  0.88802481  0.5586184  -0.2702459\n",
      "   0.52645431  0.69598575 -0.83457721 -0.16256426 -0.03058141 -0.85370094\n",
      "  -0.62142908 -0.65584228 -0.27616342 -0.28745875  0.38399772  0.03179606\n",
      "  -0.3963127  -0.3897269 ]\n",
      " [-0.25548909 -0.04250944 -0.63779067  0.43110926  0.01354426  0.41525981\n",
      "   0.78275776 -0.43854834 -0.3378158   0.22975273 -0.66367269 -0.65398663\n",
      "  -0.23389283  0.31030033  0.73377394  0.80006153  0.42101434 -0.24609203\n",
      "   0.73764714 -0.79499183]\n",
      " [ 0.63784856 -0.45535006 -0.20508811 -0.52576081 -0.066436   -0.62763359\n",
      "  -0.17060312  0.63826353 -0.5522275   0.17912172  0.55201242  0.47660991\n",
      "  -0.00966091 -0.85784553 -0.52982982 -0.74105383 -0.12160335  0.46696559\n",
      "  -0.14690418 -0.8235438 ]\n",
      " [-0.49338493  0.68566274 -0.56388726  0.40896429 -0.48764451  0.26394063\n",
      "  -0.78936402  0.82523075 -0.32484156 -0.5406515   0.27797599 -0.43001904\n",
      "  -0.18325645 -0.42723789  0.54450313 -0.52744692  0.64642437  0.68221378\n",
      "   0.46616133  0.25793232]\n",
      " [-0.52512656 -0.33106947 -0.47179517  0.53252761  0.4495257  -0.67423344\n",
      "  -0.21561418 -0.54235568  0.18978661 -0.62153701 -0.76319763 -0.53044154\n",
      "   0.76182336 -0.41486711  0.70524268  0.29975176  0.27175077  0.68428338\n",
      "   0.48464723  0.65513417]\n",
      " [ 0.76604864  0.78536572 -0.69208667  0.02209129 -0.45441202 -0.33889843\n",
      "  -0.77947541 -0.66173572 -0.17887929  0.18417119 -0.36650895  0.58738758\n",
      "  -0.72983942  0.15195974  0.59661623 -0.65919578  0.15597044 -0.42193826\n",
      "   0.42998319  0.60029417]\n",
      " [ 0.65394507 -0.30925665 -0.39426571  0.73018518 -0.24907473  0.01462021\n",
      "  -0.05669463  0.64568592 -0.23348351 -0.39301236  0.18676806  0.79184714\n",
      "   0.50573257  0.55121097 -0.76954497  0.51469792  0.77587309  0.54704368\n",
      "  -0.51624787  0.50546305]]\n",
      "Sample actions:  None\n",
      "Using device: mps for ddpg\n",
      "--------------------------------------------------\n",
      "At initialization:\n",
      "Difference between actor and actor_target:  0.0\n",
      "Difference between critic1 and critic1_target:  0.0\n",
      "Difference between critic2 and critic2_target:  0.0\n",
      "Difference between critic1 and critic2:  107.12167358398438\n",
      "Difference between critic1_target and critic2_target:  107.12167358398438\n",
      "--------------------------------------------------\n",
      "[WolpertingerAgent] Using device: mps\n",
      "[WolpertingerAgent] Using 25 nearest neighbors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-21 14:38:22,775 : env: ARC\n",
      "2025-02-21 14:38:22,776 : mode: train\n",
      "2025-02-21 14:38:22,776 : id: 0\n",
      "2025-02-21 14:38:22,776 : load: True\n",
      "2025-02-21 14:38:22,777 : load_model_dir: ARC-run67\n",
      "2025-02-21 14:38:22,777 : eval_interval: 200\n",
      "2025-02-21 14:38:22,777 : eval_episodes: 2\n",
      "2025-02-21 14:38:22,777 : max_episode_length: 50\n",
      "2025-02-21 14:38:22,778 : max_episode: 20000\n",
      "2025-02-21 14:38:22,778 : max_actions: 12\n",
      "2025-02-21 14:38:22,778 : test_episode: 20\n",
      "2025-02-21 14:38:22,779 : warmup: 200\n",
      "2025-02-21 14:38:22,779 : bsize: 3\n",
      "2025-02-21 14:38:22,779 : rmsize: 100000\n",
      "2025-02-21 14:38:22,779 : gamma: 0.99\n",
      "2025-02-21 14:38:22,780 : policy_noise: 0\n",
      "2025-02-21 14:38:22,780 : noise_clip: 0.25\n",
      "2025-02-21 14:38:22,780 : policy_delay: 2\n",
      "2025-02-21 14:38:22,780 : c_lr: 0.0003\n",
      "2025-02-21 14:38:22,781 : p_lr: 0.0003\n",
      "2025-02-21 14:38:22,781 : tau_update: 0.0005\n",
      "2025-02-21 14:38:22,781 : weight_decay: 0.0001\n",
      "2025-02-21 14:38:22,781 : hidden1: 512\n",
      "2025-02-21 14:38:22,782 : hidden2: 512\n",
      "2025-02-21 14:38:22,782 : actor_critic_type: cnn\n",
      "2025-02-21 14:38:22,782 : latent_dim: 48\n",
      "2025-02-21 14:38:22,782 : epsilon: 100000\n",
      "2025-02-21 14:38:22,782 : epsilon_start: 1.0\n",
      "2025-02-21 14:38:22,783 : ou_theta: 0.5\n",
      "2025-02-21 14:38:22,783 : ou_sigma: 0.2\n",
      "2025-02-21 14:38:22,783 : ou_mu: 0.0\n",
      "2025-02-21 14:38:22,783 : gpu_ids: [1]\n",
      "2025-02-21 14:38:22,783 : gpu_nums: 8\n",
      "2025-02-21 14:38:22,784 : load_action_embedding: True\n",
      "2025-02-21 14:38:22,784 : num_experiments_filter: 120\n",
      "2025-02-21 14:38:22,784 : filter_threshold: 0.3\n",
      "2025-02-21 14:38:22,784 : num_experiments_similarity: 120\n",
      "2025-02-21 14:38:22,785 : max_embedding: 1.0\n",
      "2025-02-21 14:38:22,785 : min_embedding: -1.0\n",
      "2025-02-21 14:38:22,785 : init_w: 0.003\n",
      "2025-02-21 14:38:22,785 : seed: -1\n",
      "2025-02-21 14:38:22,785 : save_per_epochs: 25\n",
      "2025-02-21 14:38:22,786 : k_neighbors: 25\n",
      "2025-02-21 14:38:22,786 : save_model_dir: ../output/ARC-run77\n",
      "2025-02-21 14:38:22,786 : nb_states: 1805\n",
      "2025-02-21 14:38:22,786 : nb_actions: 20\n",
      "2025-02-21 14:38:22,786 : continuous: False\n",
      "2025-02-21 14:38:22,787 : Starting Training...\n",
      "2025-02-21 14:38:22,894 : [Train] Ep:0    | R: -33.00 | Steps:   12 | EqualStates:    4 | PosR:    0 | eps: 1.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actor and Critic models loaded from ARC-run67\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/episode</td><td>▁</td></tr><tr><td>train/episode_reward</td><td>▁</td></tr><tr><td>train/episode_steps</td><td>▁</td></tr><tr><td>train/epsilon</td><td>▁</td></tr><tr><td>train/positive_rewards</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train/episode</td><td>0</td></tr><tr><td>train/episode_reward</td><td>-33</td></tr><tr><td>train/episode_steps</td><td>12</td></tr><tr><td>train/epsilon</td><td>1</td></tr><tr><td>train/positive_rewards</td><td>0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">soft-lion-95</strong> at: <a href='https://wandb.ai/francesco-braicovich-bocconi-university/arc-v1/runs/cf2t9kzt' target=\"_blank\">https://wandb.ai/francesco-braicovich-bocconi-university/arc-v1/runs/cf2t9kzt</a><br> View project at: <a href='https://wandb.ai/francesco-braicovich-bocconi-university/arc-v1' target=\"_blank\">https://wandb.ai/francesco-braicovich-bocconi-university/arc-v1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250221_143820-cf2t9kzt/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from utils.util import get_output_folder, setup_logger\n",
    "from utils.util import set_device\n",
    "from wolp_agent import WolpertingerAgent\n",
    "import os\n",
    "\n",
    "# Change the current working directory to the parent folder\n",
    "os.chdir('..')\n",
    "\n",
    "# 2. Set CUDA visible devices\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(args.gpu_ids)[1:-1]\n",
    "device = set_device()\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# 3. Optionally set a process title\n",
    "ptitle('WOLP_DDPG')\n",
    "\n",
    "# 4. Prepare output folder\n",
    "args.save_model_dir = get_output_folder('../output', args.env)\n",
    "\n",
    "# 5. Initialize wandb (only if training)\n",
    "if args.mode == 'train' and wandb.run is None:\n",
    "    wandb.init(project=\"arc-v1\", config=vars(args), mode=\"online\")\n",
    "\n",
    "action_space = ARCActionSpace(args)\n",
    "action_space = ARCActionSpace(args)\n",
    "print('Sample actions: ', print(action_space))\n",
    "\n",
    "# 6. Create training and evaluation environments\n",
    "train_env = ARC_Env(\n",
    "    path_to_challenges='data/RAW_DATA_DIR/arc-prize-2024/arc-agi_training_challenges.json',\n",
    "    action_space=action_space\n",
    ")\n",
    "eval_env = ARC_Env(\n",
    "    path_to_challenges='data/RAW_DATA_DIR/arc-prize-2024/arc-agi_evaluation_challenges.json',\n",
    "    action_space=action_space\n",
    ")\n",
    "\n",
    "# 7. Set seeds\n",
    "if args.seed > 0:\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    train_env.seed(args.seed)\n",
    "    eval_env.seed(args.seed)\n",
    "\n",
    "# 8. Define state and action dimensions\n",
    "nb_states = 1805\n",
    "nb_actions = 20\n",
    "continuous = False\n",
    "\n",
    "# 9. Create the agent\n",
    "agent_args = {\n",
    "    'nb_states': nb_states,\n",
    "    'nb_actions': nb_actions,\n",
    "    'args': args,\n",
    "    'k': args.k_neighbors,\n",
    "    'action_space': action_space\n",
    "}\n",
    "agent = WolpertingerAgent(**agent_args)\n",
    "\n",
    "# 10. Optionally load model weights\n",
    "if args.load:\n",
    "    agent.load_weights(args.load_model_dir)\n",
    "\n",
    "# 11. Move agent to GPU if requested\n",
    "if args.gpu_ids[0] >= 0 and args.gpu_nums > 0 and torch.cuda.is_available():\n",
    "    agent.cuda_convert()\n",
    "\n",
    "# 12. Set up logger\n",
    "if args.mode == 'train':\n",
    "    setup_logger('RS_log', f'{args.save_model_dir}/RS_train_log')\n",
    "elif args.mode == 'test':\n",
    "    setup_logger('RS_log', f'{args.save_model_dir}/RS_test_log')\n",
    "else:\n",
    "    raise RuntimeError(f'Undefined mode {args.mode}')\n",
    "logger = logging.getLogger('RS_log')\n",
    "\n",
    "# 13. Log hyperparameters\n",
    "d_args = vars(args)\n",
    "d_args['nb_states'] = nb_states\n",
    "d_args['nb_actions'] = nb_actions\n",
    "d_args['continuous'] = continuous\n",
    "for k, v in d_args.items():\n",
    "    logger.info(f\"{k}: {v}\")\n",
    "\n",
    "# 14. Run training or (separate) test\n",
    "if args.mode == 'train':\n",
    "    logger.info('Starting Training...')\n",
    "    train(\n",
    "        continuous=continuous,\n",
    "        train_env=train_env,\n",
    "        eval_env=eval_env,\n",
    "        agent=agent,\n",
    "        max_episode=args.max_episode,\n",
    "        max_actions=args.max_actions,\n",
    "        warmup=args.warmup,\n",
    "        save_model_dir=args.save_model_dir,\n",
    "        max_episode_length=args.max_episode_length,\n",
    "        logger=logger,\n",
    "        save_per_epochs=args.save_per_epochs,\n",
    "        eval_interval=args.eval_interval,     # e.g. evaluate every 10 episodes\n",
    "        eval_episodes=args.eval_episodes     # e.g. 5 episodes each evaluation\n",
    "    )\n",
    "    # finish wandb run\n",
    "    wandb.finish()\n",
    "\n",
    "elif args.mode == 'test':\n",
    "    logger.info('Starting Testing...')\n",
    "    # You could reuse the 'evaluate' or a separate 'test(...)' function\n",
    "    from train_test import evaluate\n",
    "    evaluate(\n",
    "        agent=agent,\n",
    "        eval_env=eval_env,\n",
    "        episodes=args.test_episode,\n",
    "        max_episode_length=args.max_episode_length,\n",
    "        logger=logger\n",
    "    )\n",
    "else:\n",
    "    raise RuntimeError(f'Undefined mode {args.mode}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps for file.py\n",
      "Using device for EncoderTransformer: mps\n",
      "Using device: mps for action_space_embed.py\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Placeholder storage has not been allocated on MPS device!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 62\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPadding mask:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, mask)\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 62\u001b[0m     \u001b[43mtest_encoder_transformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 53\u001b[0m, in \u001b[0;36mtest_encoder_transformer\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m grid_shapes \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m1\u001b[39m, config\u001b[38;5;241m.\u001b[39mmax_rows \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, (B, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# Run the forward pass\u001b[39;00m\n\u001b[0;32m---> 53\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpairs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrid_shapes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncoder output shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, output\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# Test the padding mask computation\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ARCAGI2/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ARCAGI2/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Documents/GitHub/ARC2/src/learnable_embedding/action_space_embed.py:104\u001b[0m, in \u001b[0;36mEncoderTransformer.forward\u001b[0;34m(self, pairs, grid_shapes, dropout_eval)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;124;03m    pairs: shape (*B, R, C, 2), with token IDs in [0, vocab_size).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;124;03m    Encoded sequence of shape (B, T, emb_dim) where T = 1 + 4 + 2 * R * C.\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;66;03m# 1) Embed the input grids\u001b[39;00m\n\u001b[0;32m--> 104\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_grids\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpairs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrid_shapes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropout_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;66;03m# 2) Create key_padding_mask\u001b[39;00m\n\u001b[1;32m    107\u001b[0m key_padding_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_pad_mask(grid_shapes)  \u001b[38;5;66;03m# shape (B, T)\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/GitHub/ARC2/src/learnable_embedding/action_space_embed.py:136\u001b[0m, in \u001b[0;36mEncoderTransformer.embed_grids\u001b[0;34m(self, pairs, grid_shapes, dropout_eval)\u001b[0m\n\u001b[1;32m    133\u001b[0m C \u001b[38;5;241m=\u001b[39m pairs\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m    135\u001b[0m \u001b[38;5;66;03m# Embedding for color tokens: shape (B, R, C, 2) -> (B, R, C, 2, emb_dim)\u001b[39;00m\n\u001b[0;32m--> 136\u001b[0m colors_embed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolors_embed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpairs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;66;03m# Position embeddings\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mscaled_position_embeddings:\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;66;03m# pos_row_embed is nn.Embedding(1, emb_dim). We'll \"lookup\" zeros, shape -> [R, emb_dim].\u001b[39;00m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;66;03m# Then multiply by torch.arange(1, R+1).\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ARCAGI2/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ARCAGI2/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ARCAGI2/lib/python3.12/site-packages/torch/nn/modules/sparse.py:190\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ARCAGI2/lib/python3.12/site-packages/torch/nn/functional.py:2551\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2545\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2546\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2547\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2548\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2549\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2550\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2551\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Placeholder storage has not been allocated on MPS device!"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"path/to/learnable embedding\")\n",
    "\n",
    "from transformer import EncoderTransformerConfig\n",
    "from learnable_embedding.action_space_embed import EncoderTransformer\n",
    "\n",
    "\n",
    "def test_encoder_transformer():\n",
    "    # Create a dummy transformer layer configuration.\n",
    "    # Using a simple dummy config object with required attributes.\n",
    "    class DummyTransformerLayerConfig:\n",
    "        def __init__(self):\n",
    "            self.emb_dim = 32\n",
    "            self.num_heads = 4\n",
    "            self.attention_dropout_rate = 0.1\n",
    "            self.use_bias = True\n",
    "            self.dropout_rate = 0.1\n",
    "            self.activation = \"relu\"\n",
    "            self.mlp_dim_factor = 4\n",
    "\n",
    "    transformer_layer_config = DummyTransformerLayerConfig()\n",
    "    \n",
    "    # Create a dummy configuration for the EncoderTransformer.\n",
    "    config = EncoderTransformerConfig(\n",
    "        vocab_size=11,\n",
    "        max_rows=5,   # Using smaller grids for testing\n",
    "        max_cols=5,\n",
    "        emb_dim=32,\n",
    "        latent_dim=32,\n",
    "        num_layers=2,  # Two layers for testing\n",
    "        scaled_position_embeddings=False,\n",
    "        variational=False,\n",
    "        latent_projection_bias=False,\n",
    "        transformer_layer=transformer_layer_config\n",
    "    )\n",
    "    \n",
    "    # Instantiate the model\n",
    "    model = EncoderTransformer(config)\n",
    "    \n",
    "    # Create dummy input for pairs: shape (B, R, C, 2)\n",
    "    B, R, C = 2, config.max_rows, config.max_cols\n",
    "    pairs = torch.randint(0, config.vocab_size, (B, R, C, 2))\n",
    "    \n",
    "    # Create dummy input for grid_shapes: shape (B, 2, 2)\n",
    "    # For example, using numbers between 1 and max_rows (and max_cols) for both input and output shapes.\n",
    "    grid_shapes = torch.randint(1, config.max_rows + 1, (B, 2, 2))\n",
    "    \n",
    "    # Run the forward pass\n",
    "    output = model(pairs, grid_shapes, dropout_eval=False)\n",
    "    print(\"Encoder output shape:\", output.shape)\n",
    "    \n",
    "    # Test the padding mask computation\n",
    "    mask = model.make_pad_mask(grid_shapes)\n",
    "    print(\"Padding mask shape:\", mask.shape)\n",
    "    print(\"Padding mask:\\n\", mask)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_encoder_transformer()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ARCAGI2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
