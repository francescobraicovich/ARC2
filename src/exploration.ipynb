{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import logging\n",
    "from train_test import train, evaluate\n",
    "import warnings\n",
    "import json\n",
    "import wandb\n",
    "from arg_parser import init_parser\n",
    "from setproctitle import setproctitle as ptitle\n",
    "from enviroment import ARC_Env\n",
    "import gymnasium as gym\n",
    "from action_space import ARCActionSpace\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "def init_parser(alg):\n",
    "    \"\"\"Initialize argument parser for the specified algorithm.\"\"\"\n",
    "\n",
    "    if alg == 'WOLP_DDPG':\n",
    "        parser = argparse.ArgumentParser(description='WOLP_DDPG')\n",
    "\n",
    "        # Environment & Training Mode\n",
    "        parser.add_argument('--env', default='ARC', metavar='ENV', help='Environment to train on')\n",
    "        parser.add_argument('--mode', default='test', type=str, help='Mode: train/test')\n",
    "        parser.add_argument('--id', default='0', type=str, help='Experiment ID')\n",
    "        parser.add_argument('--load', default=True, metavar='L', help='Load a trained model')\n",
    "        parser.add_argument('--load-model-dir', default='ARC-run67', metavar='LMD', help='Folder to load trained models from')\n",
    "        parser.add_argument('--eval-interval', default=200, type=int, help='Evaluate model every X episodes')\n",
    "        parser.add_argument('--eval-episodes', default=2, type=int, help='Number of episodes to evaluate')\n",
    "\n",
    "        # Episode & Training Settings\n",
    "        parser.add_argument('--max-episode-length', type=int, default=50, metavar='M', help='Max episode length (default: 50)')  # Changed from 1440\n",
    "        parser.add_argument('--max-episode', type=int, default=20000, help='Maximum number of episodes')\n",
    "        parser.add_argument('--max-actions', default=12, type=int, help='# max actions')\n",
    "        parser.add_argument('--test-episode', type=int, default=20, help='Maximum testing episodes')\n",
    "        parser.add_argument('--warmup', default=200, type=int, help='Time without training but only filling the replay memory')\n",
    "        parser.add_argument('--bsize', default=3, type=int, help='Minibatch size')\n",
    "        parser.add_argument('--rmsize', default=100000, type=int, help='Replay memory size')\n",
    "\n",
    "        # Policy Update Settings\n",
    "        parser.add_argument('--gamma', type=float, default=0.99, metavar='G', help='Discount factor for rewards (default: 0.99)')\n",
    "        parser.add_argument('--policy-noise', default=0, type=float, help='Noise added to target policy during critic update')\n",
    "        parser.add_argument('--noise-clip', default=0.25, type=float, help='Range to clip target policy noise')\n",
    "        parser.add_argument('--policy-delay', default=2, type=int, help='Delay policy updates')\n",
    "        parser.add_argument('--c-lr', default=3e-4, type=float, help='Critic network learning rate')\n",
    "        parser.add_argument('--p-lr', default=3e-4, type=float, help='Policy network learning rate (for DDPG)')\n",
    "        parser.add_argument('--tau-update', default=0.0005, type=float, help='Moving average for target network')\n",
    "        parser.add_argument('--weight-decay', default=1e-4, type=float, help='L2 Regularization loss weight decay')\n",
    "        \n",
    "                            \n",
    "        # Neural Network Architecture\n",
    "        parser.add_argument('--hidden1', default=512, type=int, help='Hidden units in the first fully connected layer')\n",
    "        parser.add_argument('--hidden2', default=512, type=int, help='Hidden units in the second fully connected layer')\n",
    "        parser.add_argument('--actor_critic_type', default='cnn', type=str, help='Type of model (lpn, cnn, mlp)')\n",
    "        parser.add_argument('--latent_dim', default=48, type=int, help='Latent dimension for encoder')\n",
    "\n",
    "        # Exploration & Noise\n",
    "        parser.add_argument('--epsilon', default=100000, type=int, help='Linear decay of exploration policy')\n",
    "        parser.add_argument('--epsilon_start', default=1.0, type=float, help='Starting epsilon value for resuming training')\n",
    "        parser.add_argument('--ou_theta', default=0.5, type=float, help='Ornstein-Uhlenbeck noise theta')\n",
    "        parser.add_argument('--ou_sigma', default=0.2, type=float, help='Ornstein-Uhlenbeck noise sigma')\n",
    "        parser.add_argument('--ou_mu', default=0.0, type=float, help='Ornstein-Uhlenbeck noise mu')\n",
    "\n",
    "        # Hardware & GPU Settings\n",
    "        parser.add_argument('--gpu-ids', type=int, default=[1], nargs='+', help='GPUs to use [-1 for CPU only]')\n",
    "        parser.add_argument('--gpu-nums', type=int, default=8, help='Number of GPUs to use (default: 1)')\n",
    "\n",
    "        # Action Embedding & Filtering\n",
    "        parser.add_argument('--load_action_embedding', default=True, type=bool, help='Load action embedding or not')\n",
    "        parser.add_argument('--num_experiments_filter', default=120, type=int, help='Number of problems used for filtering actions')\n",
    "        parser.add_argument('--filter_threshold', default=0.3, type=float, help='Threshold percentage for filtering actions')\n",
    "        parser.add_argument('--num_experiments_similarity', default=120, type=int, help='Number of problems used for similarity matrix calculation')\n",
    "        parser.add_argument('--max_embedding', default=1., type=float, help='Maximum value for embedding matrix')\n",
    "        parser.add_argument('--min_embedding', default=-1., type=float, help='Minimum value for embedding matrix')\n",
    "\n",
    "        # Miscellaneous\n",
    "        parser.add_argument('--init_w', default=0.003, type=float, help='Initial weight')\n",
    "        parser.add_argument('--seed', default=-1, type=int, help='Random seed')\n",
    "        parser.add_argument('--save_per_epochs', default=25, type=int, help='Save model every X epochs')\n",
    "        parser.add_argument('--k_neighbors', default=25, type=int, help='Number of neighbors to consider')\n",
    "        return parser\n",
    "\n",
    "    else:\n",
    "        raise RuntimeError(f'Undefined algorithm {alg}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.argv = ['notebook']  # Replace with a dummy argument list\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print('Using device: {}'.format(device))\n",
    "\n",
    "\n",
    "ptitle('WOLP_DDPG')\n",
    "warnings.filterwarnings('ignore')\n",
    "parser = init_parser('WOLP_DDPG')\n",
    "args = parser.parse_args()\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(args.gpu_ids)[1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device for model: mps\n",
      "Using device for EncoderTransformer: mps\n",
      "Using device for memory: mps\n",
      "Using device: mps\n",
      "--------------------------------------------------\n",
      "Creating the action space\n",
      "Number of actions not filtered: 11310\n",
      "Number of actions filtered: 7075\n",
      "NearestNeighbors model created with 25 neighbors\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Creating the action space\n",
      "Number of actions not filtered: 11310\n",
      "Number of actions filtered: 7075\n",
      "NearestNeighbors model created with 25 neighbors\n",
      "--------------------------------------------------\n",
      "[[-0.05092603  0.62167759 -0.80313564 -0.31519486 -0.05130663 -0.35953908\n",
      "  -0.48975622 -0.55581076 -0.57480736 -0.34074036 -0.29114123  0.90078503\n",
      "   0.0063453  -0.4454143  -0.17825451 -0.05588313  0.64388515  0.86552641\n",
      "  -0.08955657  0.7753709 ]\n",
      " [-0.04885672  0.80032061  0.66708647 -0.73917153 -0.10302403  0.59080581\n",
      "   0.62320497  0.07621358 -0.72033781  0.3629343  -0.02611465  0.07564433\n",
      "  -0.53731184 -0.20741292  0.47814419 -0.88411586  0.43059644  0.67943963\n",
      "  -0.06200402 -0.48741514]\n",
      " [-0.40096055  0.24775392  0.69482523  0.75425448 -0.65625216  0.04741802\n",
      "  -0.00910887 -0.01187057  0.6116402   0.68676669  0.54557457 -0.06771539\n",
      "   0.2488829  -0.24646899 -0.08990387 -0.90780779 -0.70003807 -0.02418659\n",
      "  -0.58351821  0.79935908]\n",
      " [ 0.38637632 -0.48902757  0.35099516  0.88802481  0.5586184  -0.2702459\n",
      "   0.52645431  0.69598575 -0.83457721 -0.16256426 -0.03058141 -0.85370094\n",
      "  -0.62142908 -0.65584228 -0.27616342 -0.28745875  0.38399772  0.03179606\n",
      "  -0.3963127  -0.3897269 ]\n",
      " [-0.25548909 -0.04250944 -0.63779067  0.43110926  0.01354426  0.41525981\n",
      "   0.78275776 -0.43854834 -0.3378158   0.22975273 -0.66367269 -0.65398663\n",
      "  -0.23389283  0.31030033  0.73377394  0.80006153  0.42101434 -0.24609203\n",
      "   0.73764714 -0.79499183]\n",
      " [ 0.63784856 -0.45535006 -0.20508811 -0.52576081 -0.066436   -0.62763359\n",
      "  -0.17060312  0.63826353 -0.5522275   0.17912172  0.55201242  0.47660991\n",
      "  -0.00966091 -0.85784553 -0.52982982 -0.74105383 -0.12160335  0.46696559\n",
      "  -0.14690418 -0.8235438 ]\n",
      " [-0.49338493  0.68566274 -0.56388726  0.40896429 -0.48764451  0.26394063\n",
      "  -0.78936402  0.82523075 -0.32484156 -0.5406515   0.27797599 -0.43001904\n",
      "  -0.18325645 -0.42723789  0.54450313 -0.52744692  0.64642437  0.68221378\n",
      "   0.46616133  0.25793232]\n",
      " [-0.52512656 -0.33106947 -0.47179517  0.53252761  0.4495257  -0.67423344\n",
      "  -0.21561418 -0.54235568  0.18978661 -0.62153701 -0.76319763 -0.53044154\n",
      "   0.76182336 -0.41486711  0.70524268  0.29975176  0.27175077  0.68428338\n",
      "   0.48464723  0.65513417]\n",
      " [ 0.76604864  0.78536572 -0.69208667  0.02209129 -0.45441202 -0.33889843\n",
      "  -0.77947541 -0.66173572 -0.17887929  0.18417119 -0.36650895  0.58738758\n",
      "  -0.72983942  0.15195974  0.59661623 -0.65919578  0.15597044 -0.42193826\n",
      "   0.42998319  0.60029417]\n",
      " [ 0.65394507 -0.30925665 -0.39426571  0.73018518 -0.24907473  0.01462021\n",
      "  -0.05669463  0.64568592 -0.23348351 -0.39301236  0.18676806  0.79184714\n",
      "   0.50573257  0.55121097 -0.76954497  0.51469792  0.77587309  0.54704368\n",
      "  -0.51624787  0.50546305]]\n",
      "Sample actions:  None\n",
      "Using device: mps for ddpg\n",
      "--------------------------------------------------\n",
      "At initialization:\n",
      "Difference between actor and actor_target:  0.0\n",
      "Difference between critic1 and critic1_target:  0.0\n",
      "Difference between critic2 and critic2_target:  0.0\n",
      "Difference between critic1 and critic2:  107.23554992675781\n",
      "Difference between critic1_target and critic2_target:  107.23554992675781\n",
      "--------------------------------------------------\n",
      "[WolpertingerAgent] Using device: mps\n",
      "[WolpertingerAgent] Using 25 nearest neighbors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-20 14:32:17,285 : env: ARC\n",
      "2025-02-20 14:32:17,286 : mode: test\n",
      "2025-02-20 14:32:17,286 : id: 0\n",
      "2025-02-20 14:32:17,286 : load: True\n",
      "2025-02-20 14:32:17,286 : load_model_dir: ARC-run67\n",
      "2025-02-20 14:32:17,287 : eval_interval: 200\n",
      "2025-02-20 14:32:17,287 : eval_episodes: 2\n",
      "2025-02-20 14:32:17,287 : max_episode_length: 50\n",
      "2025-02-20 14:32:17,287 : max_episode: 20000\n",
      "2025-02-20 14:32:17,288 : max_actions: 12\n",
      "2025-02-20 14:32:17,288 : test_episode: 20\n",
      "2025-02-20 14:32:17,288 : warmup: 200\n",
      "2025-02-20 14:32:17,288 : bsize: 3\n",
      "2025-02-20 14:32:17,289 : rmsize: 100000\n",
      "2025-02-20 14:32:17,289 : gamma: 0.99\n",
      "2025-02-20 14:32:17,289 : policy_noise: 0\n",
      "2025-02-20 14:32:17,289 : noise_clip: 0.25\n",
      "2025-02-20 14:32:17,290 : policy_delay: 2\n",
      "2025-02-20 14:32:17,290 : c_lr: 0.0003\n",
      "2025-02-20 14:32:17,290 : p_lr: 0.0003\n",
      "2025-02-20 14:32:17,291 : tau_update: 0.0005\n",
      "2025-02-20 14:32:17,291 : weight_decay: 0.0001\n",
      "2025-02-20 14:32:17,291 : hidden1: 512\n",
      "2025-02-20 14:32:17,292 : hidden2: 512\n",
      "2025-02-20 14:32:17,292 : actor_critic_type: cnn\n",
      "2025-02-20 14:32:17,293 : latent_dim: 48\n",
      "2025-02-20 14:32:17,293 : epsilon: 100000\n",
      "2025-02-20 14:32:17,293 : epsilon_start: 1.0\n",
      "2025-02-20 14:32:17,293 : ou_theta: 0.5\n",
      "2025-02-20 14:32:17,294 : ou_sigma: 0.2\n",
      "2025-02-20 14:32:17,294 : ou_mu: 0.0\n",
      "2025-02-20 14:32:17,294 : gpu_ids: [1]\n",
      "2025-02-20 14:32:17,294 : gpu_nums: 8\n",
      "2025-02-20 14:32:17,295 : load_action_embedding: True\n",
      "2025-02-20 14:32:17,295 : num_experiments_filter: 120\n",
      "2025-02-20 14:32:17,295 : filter_threshold: 0.3\n",
      "2025-02-20 14:32:17,295 : num_experiments_similarity: 120\n",
      "2025-02-20 14:32:17,295 : max_embedding: 1.0\n",
      "2025-02-20 14:32:17,296 : min_embedding: -1.0\n",
      "2025-02-20 14:32:17,296 : init_w: 0.003\n",
      "2025-02-20 14:32:17,296 : seed: -1\n",
      "2025-02-20 14:32:17,296 : save_per_epochs: 25\n",
      "2025-02-20 14:32:17,297 : k_neighbors: 25\n",
      "2025-02-20 14:32:17,297 : save_model_dir: ../output/ARC-run74\n",
      "2025-02-20 14:32:17,297 : nb_states: 1805\n",
      "2025-02-20 14:32:17,297 : nb_actions: 20\n",
      "2025-02-20 14:32:17,298 : continuous: False\n",
      "2025-02-20 14:32:17,298 : Starting Testing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actor and Critic models loaded from ARC-run67\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-20 14:32:18,781 : [Eval] Ep:1/20 | Reward: -118.60 | Steps: 40 | PosR: 18 | EqualStates: 0\n",
      "2025-02-20 14:32:20,092 : [Eval] Ep:2/20 | Reward: -118.26 | Steps: 47 | PosR: 11 | EqualStates: 0\n",
      "2025-02-20 14:32:20,164 : [Eval] Ep:3/20 | Reward: -106.00 | Steps: 3 | PosR: 0 | EqualStates: 0\n",
      "2025-02-20 14:32:21,633 : [Eval] Ep:4/20 | Reward: -64.00 | Steps: 50 | PosR: 7 | EqualStates: 0\n",
      "2025-02-20 14:32:23,390 : [Eval] Ep:5/20 | Reward: -40.86 | Steps: 50 | PosR: 15 | EqualStates: 0\n",
      "2025-02-20 14:32:24,068 : [Eval] Ep:6/20 | Reward: -117.12 | Steps: 25 | PosR: 4 | EqualStates: 0\n",
      "2025-02-20 14:32:24,628 : [Eval] Ep:7/20 | Reward: -119.33 | Steps: 11 | PosR: 1 | EqualStates: 0\n",
      "2025-02-20 14:32:24,853 : [Eval] Ep:8/20 | Reward: -121.30 | Steps: 9 | PosR: 0 | EqualStates: 0\n",
      "2025-02-20 14:32:26,458 : [Eval] Ep:9/20 | Reward: -78.04 | Steps: 50 | PosR: 7 | EqualStates: 0\n",
      "2025-02-20 14:32:27,005 : [Eval] Ep:10/20 | Reward: -127.80 | Steps: 18 | PosR: 3 | EqualStates: 0\n",
      "2025-02-20 14:32:28,947 : [Eval] Ep:11/20 | Reward: -7.44 | Steps: 50 | PosR: 9 | EqualStates: 0\n",
      "2025-02-20 14:32:29,342 : [Eval] Ep:12/20 | Reward: -134.73 | Steps: 16 | PosR: 1 | EqualStates: 0\n",
      "2025-02-20 14:32:31,370 : [Eval] Ep:13/20 | Reward: -105.55 | Steps: 50 | PosR: 2 | EqualStates: 0\n",
      "2025-02-20 14:32:31,996 : [Eval] Ep:14/20 | Reward: -105.83 | Steps: 18 | PosR: 3 | EqualStates: 0\n",
      "2025-02-20 14:32:32,738 : [Eval] Ep:15/20 | Reward: -92.36 | Steps: 28 | PosR: 7 | EqualStates: 0\n",
      "2025-02-20 14:32:33,046 : [Eval] Ep:16/20 | Reward: -111.63 | Steps: 12 | PosR: 3 | EqualStates: 0\n",
      "2025-02-20 14:32:35,204 : [Eval] Ep:17/20 | Reward: -84.51 | Steps: 50 | PosR: 15 | EqualStates: 0\n",
      "2025-02-20 14:32:35,423 : [Eval] Ep:18/20 | Reward: -114.33 | Steps: 9 | PosR: 2 | EqualStates: 0\n",
      "2025-02-20 14:32:36,768 : [Eval] Ep:19/20 | Reward: 5.00 | Steps: 50 | PosR: 8 | EqualStates: 0\n",
      "2025-02-20 14:32:38,648 : [Eval] Ep:20/20 | Reward: -132.00 | Steps: 50 | PosR: 0 | EqualStates: 0\n",
      "2025-02-20 14:32:38,648 : [Eval] Average Reward over 20 episodes: -94.73\n"
     ]
    },
    {
     "ename": "Error",
     "evalue": "You must call wandb.init() before wandb.log()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mError\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 110\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;66;03m# You could reuse the 'evaluate' or a separate 'test(...)' function\u001b[39;00m\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtrain_test\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m evaluate\n\u001b[0;32m--> 110\u001b[0m     \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[43m        \u001b[49m\u001b[43magent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_env\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepisodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest_episode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_episode_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_episode_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogger\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogger\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUndefined mode \u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs\u001b[38;5;241m.\u001b[39mmode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/ARC2 with outputs/ARC2/src/train_test.py:210\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(agent, eval_env, episodes, max_episode_length, logger)\u001b[0m\n\u001b[1;32m    207\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Eval] Average Reward over \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepisodes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m episodes: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_eval_reward\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    209\u001b[0m \u001b[38;5;66;03m# wandb logging\u001b[39;00m\n\u001b[0;32m--> 210\u001b[0m \u001b[43mwandb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    211\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meval/episodes\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mepisodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meval/average_reward\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mavg_eval_reward\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[43m\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;66;03m# Switch agent back to training mode\u001b[39;00m\n\u001b[1;32m    216\u001b[0m agent\u001b[38;5;241m.\u001b[39mis_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/ARC2 with outputs/ARC2/.venv/lib/python3.11/site-packages/wandb/sdk/lib/preinit.py:36\u001b[0m, in \u001b[0;36mPreInitCallable.<locals>.preinit_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpreinit_wrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m---> 36\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m wandb\u001b[38;5;241m.\u001b[39mError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou must call wandb.init() before \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mError\u001b[0m: You must call wandb.init() before wandb.log()"
     ]
    }
   ],
   "source": [
    "from utils.util import get_output_folder, setup_logger\n",
    "from utils.util import set_device\n",
    "from wolp_agent import WolpertingerAgent\n",
    "import os\n",
    "\n",
    "# Change the current working directory to the parent folder\n",
    "os.chdir('..')\n",
    "\n",
    "# 2. Set CUDA visible devices\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(args.gpu_ids)[1:-1]\n",
    "device = set_device()\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# 3. Optionally set a process title\n",
    "ptitle('WOLP_DDPG')\n",
    "\n",
    "# 4. Prepare output folder\n",
    "args.save_model_dir = get_output_folder('../output', args.env)\n",
    "\n",
    "# 5. Initialize wandb (only if training)\n",
    "if args.mode == 'train' and wandb.run is None:\n",
    "    wandb.init(project=\"arc-v1\", config=vars(args), mode=\"online\")\n",
    "\n",
    "action_space = ARCActionSpace(args)\n",
    "action_space = ARCActionSpace(args)\n",
    "print('Sample actions: ', print(action_space))\n",
    "\n",
    "# 6. Create training and evaluation environments\n",
    "train_env = ARC_Env(\n",
    "    path_to_challenges='data/RAW_DATA_DIR/arc-prize-2024/arc-agi_training_challenges.json',\n",
    "    action_space=action_space\n",
    ")\n",
    "eval_env = ARC_Env(\n",
    "    path_to_challenges='data/RAW_DATA_DIR/arc-prize-2024/arc-agi_evaluation_challenges.json',\n",
    "    action_space=action_space\n",
    ")\n",
    "\n",
    "# 7. Set seeds\n",
    "if args.seed > 0:\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    train_env.seed(args.seed)\n",
    "    eval_env.seed(args.seed)\n",
    "\n",
    "# 8. Define state and action dimensions\n",
    "nb_states = 1805\n",
    "nb_actions = 20\n",
    "continuous = False\n",
    "\n",
    "# 9. Create the agent\n",
    "agent_args = {\n",
    "    'nb_states': nb_states,\n",
    "    'nb_actions': nb_actions,\n",
    "    'args': args,\n",
    "    'k': args.k_neighbors,\n",
    "    'action_space': action_space\n",
    "}\n",
    "agent = WolpertingerAgent(**agent_args)\n",
    "\n",
    "# 10. Optionally load model weights\n",
    "if args.load:\n",
    "    agent.load_weights(args.load_model_dir)\n",
    "\n",
    "# 11. Move agent to GPU if requested\n",
    "if args.gpu_ids[0] >= 0 and args.gpu_nums > 0 and torch.cuda.is_available():\n",
    "    agent.cuda_convert()\n",
    "\n",
    "# 12. Set up logger\n",
    "if args.mode == 'train':\n",
    "    setup_logger('RS_log', f'{args.save_model_dir}/RS_train_log')\n",
    "elif args.mode == 'test':\n",
    "    setup_logger('RS_log', f'{args.save_model_dir}/RS_test_log')\n",
    "else:\n",
    "    raise RuntimeError(f'Undefined mode {args.mode}')\n",
    "logger = logging.getLogger('RS_log')\n",
    "\n",
    "# 13. Log hyperparameters\n",
    "d_args = vars(args)\n",
    "d_args['nb_states'] = nb_states\n",
    "d_args['nb_actions'] = nb_actions\n",
    "d_args['continuous'] = continuous\n",
    "for k, v in d_args.items():\n",
    "    logger.info(f\"{k}: {v}\")\n",
    "\n",
    "# 14. Run training or (separate) test\n",
    "if args.mode == 'train':\n",
    "    logger.info('Starting Training...')\n",
    "    train(\n",
    "        continuous=continuous,\n",
    "        train_env=train_env,\n",
    "        eval_env=eval_env,\n",
    "        agent=agent,\n",
    "        max_episode=args.max_episode,\n",
    "        max_actions=args.max_actions,\n",
    "        warmup=args.warmup,\n",
    "        save_model_dir=args.save_model_dir,\n",
    "        max_episode_length=args.max_episode_length,\n",
    "        logger=logger,\n",
    "        save_per_epochs=args.save_per_epochs,\n",
    "        eval_interval=args.eval_interval,     # e.g. evaluate every 10 episodes\n",
    "        eval_episodes=args.eval_episodes     # e.g. 5 episodes each evaluation\n",
    "    )\n",
    "    # finish wandb run\n",
    "    wandb.finish()\n",
    "\n",
    "elif args.mode == 'test':\n",
    "    logger.info('Starting Testing...')\n",
    "    # You could reuse the 'evaluate' or a separate 'test(...)' function\n",
    "    from train_test import evaluate\n",
    "    evaluate(\n",
    "        agent=agent,\n",
    "        eval_env=eval_env,\n",
    "        episodes=args.test_episode,\n",
    "        max_episode_length=args.max_episode_length,\n",
    "        logger=logger\n",
    "    )\n",
    "else:\n",
    "    raise RuntimeError(f'Undefined mode {args.mode}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
